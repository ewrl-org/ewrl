---
layout: page
title: EWRL13 (2016)
permalink: /EWRL13/
nav_order: 8
---

# EWRL13 (2016) | European Workshops on Reinforcement Learning
The 13th European Workshop on Reinforcement Learning (EWRL 2016)
----------------------------------------------------------------

**Dates: December 3-4 2016**

**Location:  Pompeu Fabra University, Barcelona, Spain (co-located with [NIPS](https://nips.cc/Conferences/2016/))  
[Ramon Turró building](https://www.upf.edu/campus/en/ciutadella/turro.html) (building number 13). C/ Ramon Turró, 1 – 08005 Barcelona  
**

### [PROGRAM (pdf)](https://ewrl.wordpress.com/wp-content/uploads/2016/12/ewrlprogram.pdf)

\[[description](#describe)\] \[[submission](#submit)\] \[[dates](#dates)\] \[[committees](#committees)\] \[[keynotes](#keynotes)\] \[[papers](#accepted)\] \[[registration](#register)\] \[[venue](#venue)\] \[[schedule](#schedule)\] \[[poster sessions](#posters)\] \[[photos](#photos)\] \[[sponsors](#sponsors)\]

### Description

The 13th European workshop on reinforcement learning (EWRL 2016) invites reinforcement-learning researchers to participate in the newest edition of this world class event. We plan to make this an exciting meeting for researchers worldwide, not only for the presentation of top quality papers, but also as a forum for ample discussion of open problems and future research directions. EWRL 2016 will consist of 11+ invited talks, contributed paper presentations, discussion sessions spread over a two day period, and a poster session.

Reinforcement learning is an active field of research which deals with the problem of sequential decision making in unknown (and often) stochastic and/or partially observable environments. Recently there has been a wealth of both impressive empirical results, as well as significant theoretical advances. Both types of advances are of significant importance and we would like to create a forum to discuss such interesting results.

The workshop will cover a range of sub-topics including (but not limited to):

*   Exploration/Exploitation and multi-armed bandits
*   Deep RL
*   Representation learning for RL
*   Large-scale RL
*   Theoretical aspects of RL
*   Policy search and actor-critic methods
*   Online learning algorithms
*   RL in non-stationary environments
*   Risk-sensitive RL
*   Transfer and Multi-task RL
*   Empirical evaluations in RL
*   Kernel methods for RL
*   RL in partially observable environments
*   Imitation learning and Inverse RL
*   Bayesian RL
*   Multi agent RL
*   Applications of RL
*   Open problems

### Invited Speakers

*   [Emma Brunskill](http://www.cs.cmu.edu/~ebrun/) – _Carnegie Mellon University_
*   [Mohammad Ghavamzadeh](http://chercheurs.lille.inria.fr/~ghavamza) – _Adobe Research and INRIA_
*   [Hector Geffner](http://www.dtic.upf.edu/~hgeffner/) – _Pompeu Fabra University_
*   [John Langford](http://hunch.net/~jl/) – _Microsoft Research_
*   [Alessandro Lazaric](http://chercheurs.lille.inria.fr/~lazaric) – _INRIA_
*   [Sergey Levine](http://homes.cs.washington.edu/~svlevine/) – _University of Washington and Google_
*   [Rémi Munos](http://researchers.lille.inria.fr/~munos/) – _Google DeepMind and INRIA_
*   [Gerhard Neumann](http://www.ias.informatik.tu-darmstadt.de/Member/GerhardNeumann)– _TU Darmstadt_
*   [Ronald Ortner](http://personal.unileoben.ac.at/rortner/) – _Montanuniversität Leoben_
*   [Doina Precup](http://cs.mcgill.ca/~dprecup/) – _McGill University_
*   [Bruno Scherrer](http://iecl.univ-lorraine.fr/~Bruno.Scherrer/index_e.html) – _INRIA_
*   [Naftali Tishby](http://www.cs.huji.ac.il/~tishby/) – _The Hebrew University_

### Accepted Papers

*   _[Non-Deterministic Policy Improvement Stabilizes Approximated Reinforcement Learning.](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_2.pdf)_ Wendelin Böhmer, Rong Guo and Klaus Obermayer
*   _[Batch policy iteration algorithms for continuous domains](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_3.pdf)._ Bilal Piot, Matthieu Geist and Olivier Pietquin
*   _[Corrupt Bandits](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_04.pdf)._ Pratik Gajane, Tanguy Urvoy and Emilie Kaufmann
*   _[Exploration Potential](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_5.pdf)._ Jan Leike
*   _[Toward a data efficient neural actor-critic](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_7.pdf)._ Matthieu Zimmer, Yann Boniface and Alain Dutech
*   [_Bayesian Optimal Policies for Asynchronous Bandits with Known Trends_](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_8.pdf). Mohammed Amine Alaoui, Tanguy Urvoy and Fabrice Clérot
*   _[Online Linear Programming with Unobserved Constraints](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_9.pdf)._ Wenzhuo Yang, Shie Mannor and Huan Xu
*   [_Consistent On-Line Off-Policy Evaluation_](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_10.pdf). Assaf Hallak and Shie Mannor
*   _[Automatic Representation for Life-Time Value Recommender Systems](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_11.pdf)._ Assaf Hallak, Elad Yom-Tov and Yishay Mansour
*   _[A Lower Bound for Multi-Armed Bandits with Expert Advice](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_13.pdf)._ Yevgeny Seldin and Gábor Lugosi
*   _[Principled Option Learning in Markov Decision Processes](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_14.pdf)._ Roy Fox, Michal Moshkovitz and Naftali Tishby
*   _[Accelerated Gradient Temporal Difference Learning](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_15.pdf)._ Yangchen Pan, Adam White and Martha White
*   [_Memory Lens: How Much Memory Does an Agent Use?_](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_16.pdf) Christoph Dann, Katja Hofmann and Sebastian Nowozin
*   _[Spatio-Temporal Abstractions in Reinforcement Learning Through Neural Encoding](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_17.pdf)._ Nir Baram, Tom Zahavy and Shie Mannor
*   _[Iterative Hierarchical Optimization for Misspecified Problems](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_18.pdf)._ Daniel J. Mankowitz, Timothy Mann and Shie Mannor.
*   _[Situational Awareness by Risk-Conscious Skills](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_19.pdf)._ Daniel J. Mankowitz, Aviv Tamar and Shie Mannor
*   _[A Deep Hierarchical Approach to Lifelong Learning in Minecraft](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_20.pdf)._ Chen Tessler, Shahar Givony, Daniel J. Mankowitz, Tom Zahavy and Shie Mannor
*   _[Deep Reinforcement Learning Solutions for Energy Microgrids Management](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_21.pdf)._ Vincent Francois-Lavet, David Taralla, Damien Ernst and Raphael Fonteneau
*   _[Exploration–Exploitation in MDPs with Options](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_23.pdf)._ Ronan Fruit and Alessandro Lazaric
*   _[Using Policy Gradients to Account for Changes in Behavior Policies under Off-policy Control](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_24.pdf)._ Lucas Lehnert and Doina Precup
*   _[Linear Thompson Sampling Revisited](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_25.pdf)._ Marc Abeille and Alessandro Lazaric
*   _[Magical Policy Search: Data Efficient Reinforcement Learning with Guarantees of Global Optimality](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_26.pdf)._ Philip Thomas and Emma Brunskill
*   _[Robust Kalman Temporal Difference](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_27.pdf)._ Shirli Di-Castro Shashua and Shie Mannor
*   _[Approximations of the Restless Bandit Problem](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_28.pdf)._ Steffen Grunewalder and Azadeh Khaleghi
*   _[Decoding multitask DQN in the world of Minecraft](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission-29.pdf)._ Lydia Liu, Urun Dogan and Katja Hofmann
*   _[Why is Posterior Sampling Better than Optimism for Reinforcement Learning?](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_30.pdf)_ Ian Osband and Benjamin Van Roy
*   _[Value-Aware Loss Function for Model Learning in Reinforcement Learning](https://ewrl.wordpress.com/wp-content/uploads/2016/11/ewrl13-2016-submission_32.pdf)._ Amir-Massoud Farahmand, Andre Barreto and Daniel Nikovski

### Paper Submission

We invite submissions for the 13th European Workshop on Reinforcement Learning (EWRL 2016) from the entire reinforcement learning spectrum. Authors can submit a 2-6 pages paper in JMLR format (excluding references) that will be reviewed by the program committee in a double-blind procedure. The papers can present new work or give a summary of recent work of the author(s). All papers will be considered for the poster sessions. Outstanding long papers (4-6 pages) will also be considered for a 20 minutes oral presentation. Accepted papers are going to be published in an [arxiv.org](http://arxiv.org/) collection.

*   **Submission deadline:** 16/09/2016
*   **Page limit:** 2-6 pages excluding references.
*   **Paper format:** [JMLR format](http://www.jmlr.org/format/format.html), anonymous.
*   **Submission site:** [https://easychair.org/conferences/?conf=ewrl2016](https://easychair.org/conferences/?conf=ewrl2016)

### Important Dates

*   Paper submissions due: 16/09/2016
*   Notification of acceptance: 06/10/2016
*   Camera ready due: 11/11/2016
*   Workshop begins: 03/12/2016
*   Workshop ends: 04/12/2016

### Organizing Committee

*   [Gergely Neu](http://cs.bme.hu/~gergo/)
*   [Vicenç Gómez](http://www.mbfys.ru.nl/staff/v.gomez/)
*   [Csaba Szepesvari](https://www.ualberta.ca/~szepesva/)

### Program Committee

*   Christos Dimitrakakis
*   Marc Bellemare
*   Christian Daniel
*   Marc Deisenroth
*   Amir-massoud Farahmand
*   Victor Gabillon
*   Matthieu Geist
*   Mohammad Ghavamzadeh
*   Mohammad Gheshlaghi Azar
*   Nan Jiang
*   Anders Jonsson
*   Akshay Krishnamurthy
*   Tor Lattimore
*   Alessandro Lazaric
*   Ashique Rupam Mahmood
*   Timothy Mann
*   Jérémie Mary
*   Rémi Munos
*   Laurent Orseau
*   Ronald Ortner
*   Ian Osband
*   Bilal Piot
*   Doina Precup
*   Marcello Restelli
*   Scott Sanner
*   Georgios Theocharous
*   Michal Valko

### Keynote/Tutorial/Invited Speakers’ Abstracts

TBA

### Registration

Please follow this link to register:  
[https://goo.gl/forms/g5P9yQTnSBx92qSB3](https://goo.gl/forms/g5P9yQTnSBx92qSB3)  
Registration is free of charge, but please note that we have limited space. Thus, we kindly ask you to register only if you are really planning to participate.

### Workshop Venue

![](https://upload.wikimedia.org/wikipedia/commons/8/8d/Barcelona_collage.JPG)

EWRL13 takes place at the [Ciutadella campus](https://www.upf.edu/campus/en/ciutadella/) of the [Pompeu Fabra University](https://www.upf.edu/), in [Barcelona, Spain](https://en.wikipedia.org/wiki/Barcelona). The precise address is:

> Universitat Pompeu Fabra, [Ramon Turró building](https://www.upf.edu/campus/en/ciutadella/turro.html) (building number 13)  
> C/ Ramon Turró, 1 – 08005 Barcelona

Workshop Schedule
-----------------

### Sat 3

8:30 – 8:40 Opening remarks  
8:40 – 9:20 Invited talk: **Bruno Scherrer** (INRIA) — [On Periodic Markov Decision Processes](https://ewrl.wordpress.com/wp-content/uploads/2016/12/scherrer.pdf)  
9:20 – 9:40 Why is Posterior Sampling Better for RL? (Ian Osband)  
9:40 – 10:00 Linear Thompson Sampling Revisited (Marc Abeille)

10:00 – 10:30 Coffee break

10:30 – 11:10 Invited talk: **Héctor Geffner** (Universitat Pompeu Fabra)  
11:10 – 11:50 Invited talk: **John Langford** (Microsoft Research) — [The Contextual Reinforcement Learning Research Program](https://ewrl.wordpress.com/wp-content/uploads/2016/12/langford.pdf)  
11:50 – 12:10 A Deep Hierarchical Approach to Lifelong Learning in Minecraft (Tom Zahavy)  
12:10 – 13:30 Poster session 1

13:30 – 15:00 Lunch break (on your own)

15:00 – 15:40 Invited talk: **Remi Munos** (Google DeepMind and INRIA) — [Safe and Efficient Off-Policy Reinforcement Learning](https://ewrl.wordpress.com/wp-content/uploads/2016/12/munos.pdf)  
15:40 – 16:00 Principled Option Learning in Markov Decision Processes (Michal Moshkovitz)  
16:00 – 16:20 Exploration–Exploitation in MDPs with Options (Ronan Fruit)

16:20 – 16:50 Coffee break

16:50 – 17:30 Invited talk: **Doina Precup** (McGill University) — [How to construct good temporal abstractions](https://ewrl.wordpress.com/wp-content/uploads/2016/12/precup.pdf)  
17:30 – 18:10 Invited talk: **Ronald Ortner** (Montanuniversität Leoben) — [Some open problems for average reward MDPs](https://ewrl.wordpress.com/wp-content/uploads/2016/12/ortner.pdf)  
18:10 – 19:00 Panel discussion 1

19:00 – 21:00 Dinner break (on your own)  
21:00 – Event

### Sun 4

9:00 – 9:20 Situational Awareness by Risk-Conscious Skills (Shie Mannor)  
9:20 – 9:40 Memory Lens: How Much Memory Does an Agent Use? (Christoph Dann)  
9:40 – 10:00 Continuous LSPI (Bilal Piot)

10:00 – 10:30 Coffee break

10:30 – 11:10 Invited talk: **Mohammad Ghavamzadeh** (Adobe Research and INRIA) — [Learning Safe Policies in Sequential Decision-Making Problems](https://ewrl.wordpress.com/wp-content/uploads/2017/01/ghavamzadeh.pdf)  
11:10 – 11:50 Invited talk: **Alessandro Lazaric** (INRIA) — [Spectral Methods for Reinforcement Learning](https://ewrl.wordpress.com/wp-content/uploads/2016/12/lazaric.pdf)  
11:50 – 12:10 Accelerated Gradient Temporal Difference Learning (Martha White)  
12:10 – 13:30 Poster session 2

13:30 – 15:00 Lunch break (on your own)

15:00 – 15:40 Invited talk: **Emma Brunskill** (Carnegie Mellon University) — [Helping unlock the potential of RL](https://ewrl.wordpress.com/wp-content/uploads/2017/01/brunskill.pdf)  
15:40 – 16:00 Value-Aware Loss Function for Model Learning in Reinforcement Learning (A-M. Farahmand)  
16:00 – 16:20 Consistent On-Line Off-Policy Evaluation (Assaf Hallak)

16:20 – 16:50 Coffee break

16:50 – 17:30 Invited talk: **Sergey Levine** (University of Washington and Google) — [Challenges in Deep Reinforcement Learning](https://ewrl.wordpress.com/wp-content/uploads/2016/12/levine.pdf)  
17:30 – 18:10 Invited talk: **Gerhard Neumann (**University of Lincoln) — [Information-theoretic Policy Search Methods for Learning Versatile, Reusable Skills](https://ewrl.wordpress.com/wp-content/uploads/2016/12/neumann.pdf)  
18:10 – 19:00 Panel discussion 2

Poster sessions
---------------

**Poster session 1 (Saturday):**  
Robust Kalman Temporal Difference (Shirli Di-Castro Shashua and Shie Mannor)  
A Lower Bound for Multi-Armed Bandits with Expert Advice (Yevgeny Seldin and Gábor Lugosi)  
Magical Policy Search: Data Efficient Reinforcement Learning with Guarantees of Global Optimality (Philip Thomas and Emma Brunskill)  
Approximations of the Restless Bandit Problem (Steffen Grunewalder and Azadeh Khaleghi)  
Bayesian Optimal Policies for Asynchronous Bandits with Known Trends (Mohammed Amine Alaoui, Tanguy Urvoy and Fabrice Clérot)  
Exploration Potential (Jan Leike)  
Corrupt Bandits (Pratik Gajane, Tanguy Urvoy and Emilie Kaufmann)  
Iterative Hierarchical Optimization for Misspecified Problems (Daniel J. Mankowitz, Timothy Mann and Shie Mannor)  
A Deep Hierarchical Approach to Lifelong Learning in Minecraft _(_Chen Tessler, Shahar Givony, Daniel J. Mankowitz, Tom Zahavy and Shie Mannor)  
Situational Awareness by Risk-Conscious Skills (Daniel J. Mankowitz, Aviv Tamar and Shie Mannor)

**Poster session 2 (Sunday):**  
Decoding multitask DQN in the world of Minecraft (Lydia Liu, Urun Dogan and Katja Hofmann)  
Spatio-Temporal Abstractions in Reinforcement Learning Through Neural Encoding (Nir Baram, Tom Zahavy and Shie Mannor)  
Non-Deterministic Policy Improvement Stabilizes Approximated Reinforcement Learning (Wendelin Böhmer, Rong Guo and Klaus Obermayer)  
Automatic Representation for Life-Time Value Recommender Systems (Assaf Hallak, Elad Yom-Tov and Yishay Mansour)  
Using Policy Gradients to Account for Changes in Behavior Policies under Off-policy Control (Lucas Lehnert and Doina Precup)  
Deep Reinforcement Learning Solutions for Energy Microgrids Management (Vincent Francois-Lavet, David Taralla, Damien Ernst and Raphael Fonteneau)  
Toward a data efficient neural actor-critic (Matthieu Zimmer, Yann Boniface and Alain Dutech)

Photos
------

Sponsors
--------

![logo_png](https://ewrl.wordpress.com/wp-content/uploads/2016/03/logo_png.png?w=450)![googledeepmind-logotype-horizontal-colour-800px](https://ewrl.wordpress.com/wp-content/uploads/2016/03/googledeepmind-logotype-horizontal-colour-800px.png?w=400&h=67)

![artint_logo2_c_web_more](https://ewrl.wordpress.com/wp-content/uploads/2016/03/artint_logo2_c_web_more.jpg?w=450)
