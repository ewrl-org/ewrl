---
layout: page
title: EWRL9 (2011)
permalink: /EWRL9/
nav_order: 12
---
# EWRL9 (2011) | European Workshops on Reinforcement Learning
The 9th European Workshop on Reinforcement Learning (EWRL-9)
------------------------------------------------------------

  
**will be co-located with [ECML PKDD 2011](http://www.ecmlpkdd2011.org/)**.  
**When: Sept 9 – 11**  
**Where: Athens Greece**

\[[description](#describe)\] \[[submission](#submit)\] \[[dates](#dates)\] \[[committees](#committees)\] \[[keynotes](#keynotes)\] \[[papers](#accepted)\] \[[registration](#register)\] \[[venue](#venue)\] \[[schedule](#schedule)\] \[[photos](#photos)\] \[[sponsors](#sponsors)\]

### Description

The 9th European workshop on reinforcement learning (EWRL-9)  
invites reinforcement learning researchers to participate in  
the revival of this world class event. We plan to make this an  
exciting event for researchers worldwide, not only for the  
presentation of top quality papers, but also as a forum for  
ample discussion of open problems and future research  
directions. EWRL9 will consist of four keynote talks,  
contributed paper presentations, discussion sessions spread  
over a three day period, and a poster session with refreshments  
provided on day two.

Reinforcement learning is an active field of  
research which deals with the problem of sequential decision  
making in unknown (and often) stochastic and/or partially  
observable environments. Recently there has been a wealth of  
both impressive empirical results, as well as significant  
theoretical advances. Both types of advances are of significant  
importance and we would like to create a forum to discuss such  
interesting results.

The workshop will cover a range of sub-topics including  
(but not limited to):

*   Exploration/Exploitation
*   Function approximation in RL
*   Theoretical aspects of RL
*   Policy search methods
*   Empirical evaluations in RL
*   Kernel methods for RL
*   Partial observable RL
*   Bayesian RL
*   Multi agent RL
*   Risk-sensitive RL
*   Financial RL
*   Knowledge Representation in RL

### Keynote Speakers

*   [Peter Auer](http://personal.unileoben.ac.at/auer/) – _University of Leoben – Leoben, Austria_
*   [Kristian Kersting](http://www-kd.iai.uni-bonn.de/people.php?kristian.kersting) – _Fraunhofer IAIS, University of Bonn – Sankt Augustin, Germany_
*   [Peter Stone](http://www.cs.utexas.edu/~pstone/) – _University Of Texas – Austin, USA_
*   [Csaba Szepesvari](http://www.ualberta.ca/~szepesva/) – _University Of Alberta – Alberta, Canada_

### Paper Submission

We are calling for papers (and posters) from the entire reinforcement  
learning spectrum, with the option of either 3 page position  
papers (on which open discussion will be held) or longer 12  
page LNAI format research papers. We encourage a range of  
submissions to encourage broad discussion. Accepted papers will  
be published in the prestigious Springer LNAI proceedings.

Double submissions are allowed, however in the event that an EWRL paper is accepted to another conference proceedings or journal, copyright restrictions prevent it from being reprinted in the official EWRL Springer LNCS proceedings. The paper would still be considered, however, for acceptance and presentation at EWRL regardless of whether it can be printed in the official proceedings.

We will offer at least one best paper prize of Euro 500.

A selection of papers from EWRL-9 is to be published in the  
Springer Lecture Notes In Artificial Intelligence (LNAI/LNCS) series.

*   **Submission deadline:** June 10, 2011 June 17, 2011
*   **Page limit:** 3 pages for position papers and 12 pages for regular papers.
*   **Paper format:** LNAI Springer: [http://www.springer.de/comp/lncs/authors.html](http://www.springer.de/comp/lncs/authors.html)
*   **Paper Submissions:** Papers can be submitted [here](https://www.easychair.org/conferences/?conf=ewrl2011).  
    Please ensure papers adhere to the [Springer Lecture Notes in AI (LNAI) style](http://www.springer.de/comp/lncs/authors.html),  
    and are maximum 12 pages for long papers, or 3 pages for short papers.
*   **All submissions are to be anonymous!**

### Poster Submission

*   **Submission deadline:** 20th August, 2011
*   **Submission by email to** ewrl\_posters@yahoo.com
*   **Format:** 1 page extended abstract outlining what your poster will be about.
*   After EWRL, **all poster presenters** will have the option of submitting a 12 page version of their poster submission for consideration of acceptance to the EWRL LNCS post-proceedings.

### Important Dates

*   Paper submissions due: 10 – June – 2011 17 – June – 2011
*   Notification of acceptance: 12 – July – 2011
*   Camera ready due: 19 – July – 2011
*   Poster submission due: 20 – August – 2011
*   Workshop begins: 9 – September – 2011
*   Workshop ends: 11 – September – 2011

### Organizing Committee

*   [Marcus Hutter](http://www.hutter1.net/) (General Workshop Chair)  
    _Australian National University – Canberra, Australia_
*   [Matthew Robards](http://mrobards.wordpress.com/) (Local Organizing Chair)  
    _Australian National University – Canberra, Australia_
*   [Scott Sanner](http://users.rsise.anu.edu.au/~ssanner/) (Program Committee Chair)  
    _NICTA – Canberra, Australia_
*   [Peter Sunehag](http://people.cecs.anu.edu.au/user/1446) (Treasurer)  
    _Australian National University – Canberra, Australia_
*   [Marco Wiering](http://www.ai.rug.nl/~mwiering/) (Miscellaneous)  
    _University Of Groningen – Groningen, Netherlands_

### Program Committee

*   [Edwin Bonilla](http://users.cecs.anu.edu.au/~u4882938/) _– NICTA- Canberra, Australia_
*   [Emma Brunskill](http://www.eecs.berkeley.edu/~emma/) _– UC Berkeley – Berkeley, USA_
*   [Peter Dayan](http://www.gatsby.ucl.ac.uk/~dayan/) – _University College – London, UK_
*   [Carlos Diuk](http://www.princeton.edu/~cdiuk/) – _Princeton University – USA_
*   [Marco Dorigo](http://iridia.ulb.ac.be/~mdorigo/HomePageDorigo/) – _Université libre de Bruxelles – Brussels, Belgium_
*   [Alan Fern](http://web.engr.oregonstate.edu/~afern/)– _Oregon State University – Corvallis, USA_
*   [Fernando Fernandez](http://scalab.uc3m.es/~ffernand/) __– Universidad Carlos III de Madrid – Madrid, Spain__
*   [Mohammad Ghavamzadeh](http://chercheurs.lille.inria.fr/~ghavamza/) – _INRIA – Lille, France_
*   [Marcus Hutter](http://www.hutter1.net/) – _Australian National University – Canberra, Australia_
*   [Kristian Kersting](http://www-kd.iai.uni-bonn.de/people.php?kristian.kersting) – _Fraunhofer IAIS, University of Bonn – Bonn, Germany_
*   [Shie Manno](http://webee.technion.ac.il/people/shie/)
[r](http://webee.technion.ac.il/people/shie/) – _The Technion – Haifa, Israel_
*   [Ronald Ortner](http://personal.unileoben.ac.at/rortner/) – _Montanuniversität Leoben – Leoben, Austria_
*   [Martijn van Otterlo](http://people.cs.kuleuven.be/~martijn.vanotterlo/) – _Katholieke Universiteit Leuven – Heverlee, Belgium_
*   [Joelle Pineau](http://www.cs.mcgill.ca/~jpineau/) – _McGill University – Montreal, Canada_
*   [Doina Precup](http://www.cs.mcgill.ca/~dprecup/)– _McGill University – Montreal, Canada_
*   [Matthew Robards](http://mrobards.wordpress.com/) – _Australian National University – Canberra, Australia_
*   [Scott Sanner](http://users.rsise.anu.edu.au/~ssanner/) – _NICTA – Canberra, Australia_
*   [Juergen Schmidhuber](http://www.idsia.ch/~juergen/) – _IDSIA – Manno-Lugano Switzerland_
*   [Guy Shani](http://www.cs.bgu.ac.il/~shanigu/)– _Ben-Gurion University – Israel_
*   [David Silver](http://www.cs.ucl.ac.uk/staff/D.Silver/web/Home.html) – _University College London – UK_
*   [Peter Sunehag](http://people.cecs.anu.edu.au/user/1446) – _Australian National University – Canberra, Australia_
*   [Prasad Tadepalli](http://web.engr.oregonstate.edu/~tadepall/)– _Oregon State University – Corvallis, USA_
*   [William Uther](http://www.cse.unsw.edu.au/~willu/w/index.html) – _NICTA – Sydney, Australia_
*   [Nikos Vlassis](http://sites.google.com/site/nikosvlassis/) – _Luxembourg Centre for Systems Biomedicine – Luxembourg_
*   [Thomas Walsh](http://www.cs.arizona.edu/~twalsh/) – _Arizona State University – USA_
*   [Marco Wiering](http://www.ai.rug.nl/~mwiering/) – _University Of Groningen – Groningen, Netherlands_

### Additional Reviewers

*   [Mayank Daswani](http://people.cecs.anu.edu.au/user/4025/) – _Australian National University – Canberra, Australia_
*   [Shivaram Kalyanakrishnan](http://www.cs.utexas.edu/~shivaram/) – _University of Texas, Austin – Austin, TX, USA_
*   [Tor Lattimore](http://people.cecs.anu.edu.au/user/4102) – _Australian National University – Canberra, Australia_
*   [Phuong Minh Nguyen](http://nmphuong.wordpress.com/) – _Australian National University – Canberra, Australia_
*   [Wén Shào](http://people.cecs.anu.edu.au/user/4416/) – _Australian National University – Canberra, Australia_
*   [Daniel Visentin](http://people.cecs.anu.edu.au/user/4020) – _Australian National University – Canberra, Australia_
*   [Monica Vroman](http://paul.rutgers.edu/~babes/) _– Rutgers University – Piscataway, NJ, USA_

Keynote Speakers’ Abstracts
---------------------------

### [Peter Auer](http://personal.unileoben.ac.at/auer/) – _University of Leoben – Leoben, Austria_

**UCRL and autonomous exploration**

After reviewing the main ingredients of the UCRL algorithm and its  
analysis for online reinforcement learning – exploration vs.  
exploitation, optimism in the face of uncertainty, consistency with  
observations and upper confidence bounds, regret analysis – I show how  
these techniques can also be used to derive PAC-MDP bounds which match  
the best currently available bounds for the discounted and the  
undiscounted setting. As typical for reinforcement learning, the  
analysis for the undiscounted setting is significantly more involved.

In the second part of my talk I consider a model for autonomous  
exploration, where an agent learns about its environment and how to  
navigate in it. Whereas evaluating autonomous exploration is typically  
difficult, in the presented setting rigorous performance bounds can be  
derived. For that we present an algorithm that optimistically explores,  
by repeatedly choosing the apparently closest unknown state – as  
indicated by an optimistic policy – for further exploration.

This talk is based on joint works with Shiau Hong Lim.  
The research leading to these results has received funding from the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement 231495 (CompLACS).

### [Kristian Kersting](http://www-kd.iai.uni-bonn.de/people.php?kristian.kersting) – _Fraunhofer IAIS, University of Bonn – Bonn, Germany_

**Increasing Representational Power and Scaling Inference in Reinforcement Learning**

As robots are starting to perform everyday manipulation tasks,  
such as cleaning up, setting a table or preparing simple meals,  
they must become much more knowledgeable than they are today.  
Natural environments are composed of objects, and the possibilities  
to manipulate them are highly structured due to the general  
laws governing our relational world. All these need to be  
acknowledged when we want to realize thinking robots that efficiently  
learn how to accomplish tasks in our relational world.

Triggered by this grand vision, this talk discusses the very promising  
perspective on the application of Statistical Relational AI techniques  
to reinforcement learning. Specifically, it reviews existing symbolic  
dynamic programming and relational RL approaches that exploit the symbolic  
structure in the solution of relational and first-order logical Markov  
decision processes. They illustrate that Statistical Relational AI may  
give new tools for solving the ‘scaling challenge’. It is sometimes  
mentioned that scaling RL to real-world scenarios is a core  
challenge for robotics and AI in general. While this is true in a trivial  
sense, it might be beside the point. Reasoning and learning on appropriate  
(e.g. relational) representations leads to another view on the  
‘scaling problem’: often we are facing problems with symmetries not  
reflected in the structure used by our standard solvers. As additional  
evidence for this, the talk concludes by presenting our ongoing work on  
the first lifted linear programming solvers for MDPs. Given an MDP, our  
approach first constructs a lifted program where each variable presents a  
set of original variables that are indistinguishable given the objective  
function and constraints. It then runs any standard LP solver on this  
program to solve the original program optimally.

This talk is based on joint works with Babak Ahmadi, Kurt Driessens,  
Saket Joshi, Roni Khardon, Tobias Lang, Martin Mladenov, Sriraam Natarajan,  
Scott Sanner, Jude Shavlik, Prasad Tadepalli, and Marc Toussaint.

### [Peter Stone](http://www.cs.utexas.edu/~pstone/) – _University Of Texas – Austin, USA_

**PRISM – Practical RL: Representation, Interaction, Synthesis, and** **Mortality**

When scaling up RL to large continuous domains with imperfect  
representations and hierarchical structure, we often try applying  
algorithm that are proven to converge in small finite domains, and  
then just hope for the best. This talk will advocate instead  
designing algorithms that adhere to the constraints, and indeed take  
advantage of the opportunities, that might come with the problem at  
hand. Drawing on several different research threads within the  
Learning Agents Research Group at UT Austin, I will discuss four types  
of issues that arise from these contraints and opportunities: 1)  
Representation – choosing the algorithm for the problem’s  
representation and adapating the representation to fit the algorithm;  
2) Interaction – with other agents and with human trainers; 3)  
Synthesis – of different algorithms for the same problem and of  
different concepts in the same algorithm; and 4) Mortality – the  
opportunity to improve learning based on past experience and the  
constraint that one can’t explore exhaustively.

### [Csaba Szepesvari](http://www.ualberta.ca/~szepesva/) – _University Of Alberta – Alberta, Canada_

**Towards robust reinforcement learning algorithms**

Most reinforcement learning algorithms assume that the system to be controlled can be accurately approximated given the measurements and the available resources. However, this assumption is overly optimistic for too many problems of practical interest: Real-world problems are messy. For example, the number of unobserved variables influencing the dynamics can be very large and the dynamics governing can be highly complicated. How can then one ask for near-optimal performance without requiring an enormous amount of data? In this talk we explore an alternative to this standard criterion, based on the concept of regret, borrowed from the online learning literature. Under this alternative criterion, the performance of a learning algorithm is measured by how much total reward is collected by the algorithm as compared to the total reward that could have been collected by the best policy from a fixed policy class, the best policy being determined in hindsight. How can we design algorithms that keep the regret small? Do we need to change existing algorithm designs? In this talk, following the initial steps made by Even-Dar et al. and Yu et al., I will discuss some of our new results that shed some light on these questions.

The talk is based on joint work with Gergely Neu, Andras Gyorgy and Andras Antos.

Accepted Papers
---------------

The following is a list of presentations which will be made at EWRL9.

*   Phuong Nguyen, Peter Sunehag and Marcus Hutter – [_Feature Reinforcement Learning in Practice_](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_110.pdf)
*   Pablo Castro and Doina Precup – _[Automatic construction of temporally extended actions for MDPs using bisimulation metrics](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_2.pdf)_
*   Kazuteru Miyazaki and Masaaki Ida – _[Proposal and Evaluation of the Active Course Classification Support System with Exploitation-oriented](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_3.pdf) Learning_
*   Seiya Kuroda, Kazuteru Miyazaki and Hiroaki Kobayashi – _[Introduction of Fixed Mode States into Online Profit Sharing and Its Application to Waist Trajectory Generation of Biped Robot](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_4.pdf)_
*   Matthew Robards and Peter Sunehag – _[Near Optimal On-Policy Control](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_5.pdf)_
*   Orly Avner and Shie Mannor – _[Stochastic Bandits with Pathwise Constraints](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_6.pdf)_
*   Soumi Ray and Tim Oates – _[Locking in Returns: Speeding Up Q-Learning by Scaling](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_7.pdf)_
*   Matthew Robards and Peter Sunehag – _[Loss Functions For Improved On-Policy Control](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_8.pdf)_
*   Ioannis Lambrou, Vassilis Vassiliades and Chris Christodoulou – _[An extension of a hierarchical reinforcement learning algorithm for multiagent settings](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_9.pdf)_
*   Dimitris Kalles and Panagiotis Kanellopoulos – _[A Pendulum Effect in Co-evolutionary Learning in Games](https://ewrl.wordpress.com/wp-content/uploads/2011/09/ewrl2011_paper10.pdf)_
*   Yuxi Li and Dale Schuurmans – _[MapReduce for Parallel Reinforcement Learning](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_11.pdf)_
*   Mohammad Ghavamzadeh, Alessandro Lazaric, Remi Munos and Matthew Hoffman – _[Finite-Sample Analysis of Lasso-TD](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_13.pdf)_
*   Francis Maes, Louis Wehenkel and Damien Ernst – _[Optimized look-ahead tree search policies](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_14.pdf)_
*   Francis Maes, Louis Wehenkel and Damien Ernst – _[Automatic discovery of ranking formulas for playing with multi-armed bandits](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_15.pdf)_
*   Yann-Michaël De Hauwere, Peter Vrancx and Ann Nowé – _[Future sparse interactions: a MARL approach](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_16.pdf)_
*   Mauricio Araya-López, Olivier Buffet, Vincent Thomas and François Charpillet – _[Active Learning of MDP models](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_17.pdf)_
*   Edouard Klein, Matthieu Geist and Olivier Pietquin – _[Batch, Off-policy and Model-free Apprenticeship Learning](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_18.pdf)_
*   Georgios Boutsioukis, Ioannis Partalas and Ioannis Vlahavas – [Transfer Learning in Multi-agent Reinforcement Learning Domains](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_19.pdf)
*   Christos Dimitrakakis – _[Robust Bayesian reinforcement learning through tight lower bounds](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_20.pdf)_
*   Kfir Levy and Nahum Shimkin – _[Unified Inter and Intra Options Learning Using Policy Gradient Methods](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_21.pdf)_
*   Bruno Scherrer and Matthieu Geist – _[Recursive Least-Squares Learning with Eligibility Traces](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_22.pdf)_
*   Christos Dimitrakakis and Constantin Rothkopf – _[Bayesian multitask inverse reinforcement learning](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_23.pdf)_
*   Abdel Rodriguez Abed, Matteo Gagliolo, Peter Vrancx, Ricardo Grau and Ann Nowe – _[Improving the performance of Continuous Action Reinforcement Learning Automata](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_24.pdf)_
*   Kyriakos Chatzidimitriou, Ioannis Partalas, Pericles Mitkas and Ioannis Vlahavas – _[Transferring Evolved Reservoir Features in Reinforcement Learning Tasks](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_25.pdf)_
*   Nikolaos Tziortziotis and Konstantinos Blekas – _[Value Function Approximation through Sparse Bayesian Modeling](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_26.pdf)_
*   Boris Lesner and Bruno Zanuttini – _[Handling Ambiguous Effects in Action Learning](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_27.pdf)_
*   Charles Elkan – _[Reinforcement learning with a bilinear Q function](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_28.pdf)_
*   Adrien Couetoux and Hassen Doghmen – _[Adding Double Progressive Widening to Upper Confidence Tree to Cope with Uncertainty in Planning Problems](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_29.pdf)_
*   Lutz Frommberger – _[Task Space Tile Coding: In-Task and Cross-Task Generalization in Reinforcement Learning](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_30.pdf)_
*   Matthijs Snel and Shimon Whiteson – _[Multi-Task Reinforcement Learning: Shaping and Feature Selection](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_31.pdf)_
*   Tohgoroh Matsui, Takashi Goto, Kiyoshi Izumi and Yu Chen – _[Compound Reinforcement Learning: Theory and An Application to Finance](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_33.pdf)_
*   Cosmin Paduraru, Doina Precup and Joelle Pineau – _[A Framework for Computing Bounds for the Return of a Policy](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_34.pdf)_
*   Matthew Hoffman, Alessandro Lazaric, Mohammad Ghavamzadeh and Remi Munos – _[Regularized Least Squares Temporal Difference learning with nested l2 and l1 penalization](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_35.pdf)_
*   Matteo Leonetti, Luca Iocchi and Subramanian Ramamoorthy – _[Learning Finite State Controllers from Simulation](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_361.pdf)_
*   Kvn Pradyot and Balaraman Ravindran – _[Beyond Rewards: Learning from richer supervision](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_37.pdf)_
*   Lewis Fishgold – _[Towards Online Learning of Noisy Deictic Action Models](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_38.pdf)_
*   Anestis Fachantidis, Ioannis Partalas, Matthew Taylor and Ioannis Vlahavas – _[Transfer Learning via Multiple Inter-Task Mappings](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_39.pdf)_
*   Sylvie Ong, Yuri Grinberg and Joelle Pineau – [_Goal-Directed Online Learning of Predictive Models_](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_40.pdf)
*   Munu Sai and Balaraman Ravindran – _[Options With Exceptions](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_41.pdf)_

Registration
------------

We are pleased to announce that registration for EWRL9 is free!  
Please simply send the following details to **ewrl\_registration <at> yahoo.com:**

*   Full Name:
*   Email Address:
*   Home Institution:
*   Country:
*   Are you a student (this is simply for our records)?:
*   Do you intend to present a poster?:

(Note that poster presentation is not obligatory, however we would like to encourage all attendees to take the opportunity to present a poster at our fun poster evening. This evening will include free food and drinks.)

Workshop Venue
--------------

![](https://i0.wp.com/www.ecmlpkdd2011.org/images/venue.jpg)  
[Athens Royal Olympic Hotel](http://www.royalolympic.com/)  
EWRL9 is co-located with ECML PKDD 2011. It is to be held at Athens Royal Olympic Hotel, which is a family run five star property in the centre of Athens. It lays just in front of the famous Temple of Zeus and the National Gardens. It is underneath the Acropolis and only 2 minutes walk to the new Athens Acropolis Museum.  
After its complete renovation that finished in 2009, the Royal Olympic was transformed to an art hotel very elegantly decorated and more important very well looked after in every detail. One of the aspects given particular attention to, was to create a very personal hotel and as much environmentally friendly as possible.

Workshop Schedule
-----------------

### Day 1 – Sept 09:

*   [Welcome](https://ewrl.wordpress.com/wp-content/uploads/2011/09/ewrl2011-welcome-slides.pdf) (0900 – 0930)
*   Session 1 – **Online Learning in RL 1 (0930 – 1030)**

*   Francis Maes, Louis Wehenkel and Damien Ernst – [Automatic discovery of ranking formulas for playing with multi-armed bandits](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_15.pdf)
*   Lewis Fishgold – [Towards Online Learning of Noisy Deictic Action Models](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_38.pdf)
*   Sylvie Ong, Yuri Grinberg and Joelle Pineau – [Goal-Directed Online Learning of Predictive Models](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_40.pdf)

*   Coffee Break (1030 – 1100)
*   Session 2 – **Online Learning in RL 2 (1100 – 1140)**

*   Matthew Robards and Peter Sunehag – [Near Optimal On-Policy Control](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_5.pdf)

*   Matthew Robards and Peter Sunehag – [Loss Functions For Improved On-Policy Control](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_8.pdf)

*   Lunch – Not Provided (1140 – 1300)
*   Session 3 – **Invited talk (1300 – 1400)**

*   Csaba Szepesvari – Towards robust reinforcement learning algorithms

*   Session 4 – **Multi-Agent Reinforcement Learning (1400 – 1520)**

*   Ioannis Lambrou, Vassilis Vassiliades and Chris Christodoulou – [An extension of a hierarchical reinforcement learning algorithm for multiagent settings](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_9.pdf)
*   Dimitris Kalles and Panagiotis Kanellopoulos – [A Pendulum Effect in Co-evolutionary Learning in Games](https://ewrl.wordpress.com/wp-content/uploads/2011/09/ewrl2011_paper10.pdf)
*   Yann-Michaël De Hauwere, Peter Vrancx and Ann Nowé – [Future sparse interactions: a MARL approach](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_16.pdf)
*   Georgios Boutsioukis, Ioannis Partalas and Ioannis Vlahavas – [Transfer Learning in Multi-agent Reinforcement Learning Domains](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_19.pdf)

*   Coffee Break (1520 – 1550)
*   Session 5 – **Learning And Exploring MDPs (1550 – 1720)**

*   Phuong Nguyen, Peter Sunehag and Marcus Hutter – [Feature Reinforcement Learning in Practice](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_110.pdf)
*   Soumi Ray and Tim Oates – [Locking in Returns: Speeding Up Q-Learning by Scaling](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_7.pdf)
*   Mauricio Araya-López, Olivier Buffet, Vincent Thomas and François Charpillet – [Active Learning of MDP models](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_17.pdf)
*   Christos Dimitrakakis – [Robust Bayesian reinforcement learning through tight lower bounds](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_20.pdf)
*   Boris Lesner and Bruno Zanuttini – [Handling Ambiguous Effects in Action Learning](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_27.pdf)

### Day 2 – Sept 10:

*   Session 1 – Invited Talk (0900 – 1000)

*   Peter Auer – UCRL and autonomous exploration

*   Session 2 – **Function Approximation Methods For Reinforcement Learning 1 (1000 – 1040)**

*   Mohammad Ghavamzadeh, Alessandro Lazaric, Remi Munos and Matthew Hoffman – [Finite-Sample Analysis of Lasso-TD](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_13.pdf)
*   Bruno Scherrer and Matthieu Geist – [Recursive Least-Squares Learning with Eligibility Traces](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_22.pdf)

*   Coffee Break (1040 – 1100)
*   Session 3 – **Function Approximation Methods For Reinforcement Learning 2 (1100 – 1220)**

*   Abdel Rodriguez Abed, Matteo Gagliolo, Peter Vrancx, Ricardo Grau and Ann Nowe – [Improving the performance of Continuous Action Reinforcement Learning Automata](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_24.pdf)
*   Nikolaos Tziortziotis and Konstantinos Blekas – [Value Function Approximation through Sparse Bayesian Modeling](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_26.pdf)
*   Charles Elkan – [Reinforcement learning with a bilinear Q function](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_28.pdf)
*   Matthew Hoffman, Alessandro Lazaric, Mohammad Ghavamzadeh and Remi Munos – [Regularized Least Squares Temporal Difference learning with nested l2 and l1 penalization](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_35.pdf)

*   Lunch – Not provided (1220 – 1400)

*   Session 4 – **Best Paper Presentation (1400 – 1430)**
*   Session 5 – **Invited Talk (1430 – 1530)**

*   Kristian Kersting – Increasing Representational Power and Scaling Inference in Reinforcement Learning

*   Coffee Break (1530 – 1600)
*   Session 6 – **Macro-Actions in Reinforcement Learning (1600 – 1700)**

*   Pablo Castro and Doina Precup – [Automatic construction of temporally extended actions for MDPs using bisimulation metrics](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_2.pdf)
*   Kfir Levy and Nahum Shimkin – [Unified Inter and Intra Options Learning Using Policy Gradient Methods](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_21.pdf)
*   Munu Sai and Balaraman Ravindran – [Options With Exceptions](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_41.pdf)

*   Business Meeting (1730 – 1800)

*   Please feel free to stay around and discuss the future of EWRL

*   Poster Evening (1830 – 2130)

*   Drinks and fingerfoods will be provided.

### Day 3 – Sept 11:

*   Session 1 – Invited Talk (0900 – 1000)

*   Peter Stone – PRISM – Practical RL: Representation, Interaction, Synthesis, and  
    Mortality

*   Coffee Break (1000 – 1030)
*   Session 3 – **Policy Search Methods 2 (1030 – 1150)**

*   Adrien Couetoux and Hassen Doghmen – [Adding Double Progressive Widening to Upper Confidence Tree to Cope with Uncertainty in Planning Problems](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_29.pdf)
*   Francis Maes, Louis Wehenkel and Damien Ernst – [Optimized look-ahead tree search policies](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_14.pdf)
*   Cosmin Paduraru, Doina Precup and Joelle Pineau – [A Framework for Computing Bounds for the Return of a Policy](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_34.pdf)
*   Matteo Leonetti, Luca Iocchi and Subramanian Ramamoorthy – [Learning Finite State Controllers from Simulation](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_361.pdf)

*   Lunch – Not provided (1150 – 1330)
*   Session 4 – **Multi-Task and Transfer Learning in RL (1330 – 1450)**

*   Kyriakos Chatzidimitriou, Ioannis Partalas, Pericles Mitkas and Ioannis Vlahavas – [Transferring Evolved Reservoir Features in Reinforcement Learning Tasks](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_25.pdf)
*   Lutz Frommberger – [Task Space Tile Coding: In-Task and Cross-Task Generalization in Reinforcement Learning](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_30.pdf)
*   Matthijs Snel and Shimon Whiteson – [Multi-Task Reinforcement Learning: Shaping and Feature Selection](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_31.pdf)
*   Anestis Fachantidis, Ioannis Partalas, Matthew Taylor and Ioannis Vlahavas – [Transfer Learning via Multiple Inter-Task Mappings](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_19.pdf)

*   Coffee Break (1450 – 1520)
*   Session 5 – **Learning With Supervision (1520 – 1620)**

*   Edouard Klein, Matthieu Geist and Olivier Pietquin – [Batch, Off-policy and Model-free Apprenticeship Learning](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_18.pdf)
*   Christos Dimitrakakis and Constantin Rothkopf – [Bayesian multitask inverse reinforcement learning](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_23.pdf)
*   Kvn Pradyot and Balaraman Ravindran – [Beyond Rewards: Learning from richer supervision](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_37.pdf)

*   Session 6 – **Real World Reinforcement Learning (1620 – 1740)**

*   Seiya Kuroda, Kazuteru Miyazaki and Hiroaki Kobayashi – [Introduction of Fixed Mode States into Online Profit Sharing and Its Application to Waist Trajectory Generation of Biped Robot](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_4.pdf)
*   Kazuteru Miyazaki and Masaaki Ida – [Proposal and Evaluation of the Active Course Classification Support System with Exploitation-oriented Learning](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_3.pdf)
*   Yuxi Li and Dale Schuurmans – [MapReduce for Parallel Reinforcement Learning](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_11.pdf)
*   Tohgoroh Matsui, Takashi Goto, Kiyoshi Izumi and Yu Chen – [Compound Reinforcement Learning: Theory and An Application to Finance](https://ewrl.wordpress.com/wp-content/uploads/2011/08/ewrl2011_submission_33.pdf)

*   Close (1740 – 1800)

Photos
------

![](https://ewrl.wordpress.com/wp-content/uploads/2011/09/ewrl9a-csaba-szepesvari.jpg?w=450)  
**Keynote speaker Csaba Szepesvari’s world view.**

![](https://ewrl.wordpress.com/wp-content/uploads/2011/09/ewrl9b-audience.jpg?w=450)  
**The audience listening in awe.**

![](https://ewrl.wordpress.com/wp-content/uploads/2011/09/ewrl9c-peter-aurer.jpg?w=450)  
**Keynote speaker Peter Auer’s regret is bounded.**

![](https://ewrl.wordpress.com/wp-content/uploads/2011/09/ewrl9d-peter-aurer.jpg?w=450)  
**The audience tries to follow his proof.**

![](https://ewrl.wordpress.com/wp-content/uploads/2011/09/ewrl9e-audience.jpg?w=450)  
**The organizers are all ears.**

![](https://ewrl.wordpress.com/wp-content/uploads/2011/09/ewrl9f-kristian-kertsting.jpg?w=450)  
**Keynote speaker Kristian Kersting indulges in exponential progress.**

![](https://ewrl.wordpress.com/wp-content/uploads/2011/09/ewrl9g-peter-stone.jpg?w=450)  
**Keynote speaker Peter Stone’s heavy traffic vision: 12-lanes and 4-way green light.**

![Poster Evening](https://ewrl.wordpress.com/wp-content/uploads/2011/09/img_0434.jpg?w=450)  
**Such bold vision requires some lighter refreshments at the buffet.**  
![Poster Evening](https://ewrl.wordpress.com/wp-content/uploads/2011/09/img_0446.jpg?w=450)

![](https://ewrl.wordpress.com/wp-content/uploads/2011/09/ewrl9i-posters.jpg?w=450)  
**Lively discussion at the poster evening.**

![](https://ewrl.wordpress.com/wp-content/uploads/2011/09/ewrl9j-posters.jpg?w=450)  
**Lively discussion at the poster evening.**

![](https://ewrl.wordpress.com/wp-content/uploads/2011/09/ewrl9k-posters.jpg?w=450)  
**Lively discussion at the poster evening.**

Sponsors
--------

We thank the following sponsors for their generous support which allowed us to make the workshop accessible to everyone.  

[![ANU](https://ewrl.wordpress.com/wp-content/uploads/2011/06/aij_cover.gif?w=200)](http://www.aijd.org/)

[![ANU](https://ewrl.wordpress.com/wp-content/uploads/2011/04/anu_logo3.png?w=400)](http://cecs.anu.edu.au/rsise)

[![ANU](https://ewrl.wordpress.com/wp-content/uploads/2011/03/nictalogo-sidebyside60.jpg?w=400)](http://www.nicta.com.au/)

[![ANU](https://ewrl.wordpress.com/wp-content/uploads/2011/04/pascal2sm.png?w=400)](http://www.pascal-network.org/)

[![Springer](https://ewrl.wordpress.com/wp-content/uploads/2011/10/springerlogo.jpeg?w=400)](http://www.springer.com/lncs)
