---
layout: page
title: EWRL10 (2012)
permalink: /EWRL10/
nav_order: 11
---
# EWRL10 (2012) | European Workshops on Reinforcement Learning
The 10th European Workshop on Reinforcement Learning (EWRL 2012)
----------------------------------------------------------------

  
**Dates: June 30 – July 1 2012** (2-days workshop @ [ICML 2012](http://icml.cc/2012/))  
**Location: Edinburgh, Scotland (2-days ICML Workshop)  
Post-Workshop Proceedings: JMLR W&C Proceedings, Vol. 24  
**

\[[dates](#dates)\] \[[submission](#submit)\] \[[committees](#pc)\] \[[keynotes](#keynotes)\] \[[papers](#papers)\] \[[registration](#registration)\] \[[venue](#venue)\] \[[schedule](#schedule)\] \[[sponsors](#sponsors)\]

### Important Dates

Conference: June 30 – July 1 2012 (@ICML)

### Keynote Speakers

Shie Mannor (Technion)  
Rich Sutton (University of Alberta)  
Martin Riedmiller (University of Freiburg)  
Drew Bagnell (Carnegie Mellon University)

### Organizing Committee

Marc Deisenroth (TU Darmstadt)  
Csaba Szepesvari (University of Alberta)  
Jan Peters (TU Darmstadt)

Proceedings of the Tenth European Workshop on Reinforcement Learning

June, 2012, Edinburgh, Scotland


-------------------------------------------------------------------------------------------------------

**Editors: Marc Peter Deisenroth, Csaba Szepesvári, Jan Peters**  



*  Preface Marc Peter Deisenroth, Csaba Szepesvári, Jan Peters  [pdf]:  Learning Exploration/Exploitation Strategies for Single Trajectory Reinforcement Learning Michael Castronovo, Francis Maes, Raphael Fonteneau, Damien Ernst   ; JMLR W&C 24:1-10, 2012. [abs][pdf]
*  Preface Marc Peter Deisenroth, Csaba Szepesvári, Jan Peters  [pdf]: Feature Reinforcement Learning using Looping Suffix Trees Mayank Daswani, Peter Sunehag, Marcus Hutter  ; JMLR W&C 24:11-24, 2012. [abs][pdf]
*  Preface Marc Peter Deisenroth, Csaba Szepesvári, Jan Peters  [pdf]: Planning in Reward-Rich Domains via PAC Bandits Sergiu Goschin, Ari Weinstein, Michael L. Littman, Erick Chastain ; JMLR W&C 24:25-42, 2012. [abs][pdf]
*  Preface Marc Peter Deisenroth, Csaba Szepesvári, Jan Peters  [pdf]: Actor-Critic Reinforcement Learning with Energy-Based Policies Nicolas Heess, David Silver, Yee Whye Teh ; JMLR W&C 24:43-58, 2012. [abs][pdf]
*  Preface Marc Peter Deisenroth, Csaba Szepesvári, Jan Peters  [pdf]: Directed Exploration in Reinforcement Learning with Transferred Knowledge Timothy A. Mann, Yoonsuck Choe ; JMLR W&C 24:59-76, 2012. [abs][pdf]
*  Preface Marc Peter Deisenroth, Csaba Szepesvári, Jan Peters  [pdf]: Online Skill Discovery using Graph-based Clustering Jan Hendrik Metzen ; JMLR W&C 24:77-88, 2012. [abs][pdf]
*  Preface Marc Peter Deisenroth, Csaba Szepesvári, Jan Peters  [pdf]: An Empirical Analysis of Off-policy Learning in Discrete MDPs Cosmin Păduraru, Doina Precup, Joelle Pineau, Gheorghe Comănici ; JMLR W&C 24:89-102, 2012. [abs][pdf]
*  Preface Marc Peter Deisenroth, Csaba Szepesvári, Jan Peters  [pdf]: Evaluation and Analysis of the Performance of the EXP3 Algorithm in Stochastic Environments Yevgeny Seldin, Csaba Szepesvári, Peter Auer, Yasin Abbasi-Yadkori ; JMLR W&C 24:103-116, 2012. [abs][pdf]
*  Preface Marc Peter Deisenroth, Csaba Szepesvári, Jan Peters  [pdf]: Gradient Temporal Difference Networks David Silver ; JMLR W&C 24:117-130, 2012. [abs][pdf]
*  Preface Marc Peter Deisenroth, Csaba Szepesvári, Jan Peters  [pdf]: Semi-Supervised Apprenticeship LearningMichal Valko, Mohammad Ghavamzadeh, Alessandro Lazaric ; JMLR W&C 24:131-142, 2012. [abs][pdf]
*  Preface Marc Peter Deisenroth, Csaba Szepesvári, Jan Peters  [pdf]: An investigation of imitation learning algorithms for structured predictionAndreas Vlachos ; JMLR W&C 24:143-154, 2012. [abs][pdf]
*  Preface Marc Peter Deisenroth, Csaba Szepesvári, Jan Peters  [pdf]: Rollout-based Game-tree Search Outprunes Traditional Alpha-betaAri Weinstein, Michael L. Littman, Sergiu Goschin ; JMLR W&C 24:155-167, 2012. [abs][pdf]


Paper Submission
----------------

We are calling for papers (and posters) from the entire reinforcement learning spectrum, with the option of either 2 page position papers (on which open discussion will be held) or longer 8 page JMLR format research papers. We encourage a range of submissions to encourage broad discussion. We will publish a selection of accepted papers in the prestigious JMLR W&C Proceedings.

**Double submissions** (e.g., with ICML) are OK. However in the event that an EWRL paper is accepted to another conference proceedings or journal, it will not be reprinted in the official EWRL proceedings (JMLR W&C). The paper would still be considered, however, for acceptance and presentation at EWRL regardless of whether it can be printed in the official proceedings. Double submissions must be clearly labelled as such (e.g., add a footnote on the first page). In case your ICML submission exceeds EWRL’s page limit, don’t worry too much about it: submit the ICML paper.

We will publish a selection of papers from EWRL 2012 in the JMLR Workshop & Conference Proceedings

*   **Page limit:** 2 pages for short papers and 8 pages for regular papers (plus references).
*   **Paper format:** [JMLR W&C style](http://www.tex.ac.uk/tex-archive/help/Catalogue/entries/jmlr.html)
*   Papers for the JMLR W&C Proceedings must be **resubmitted after EWRL.**  
    Details after EWRL.

Registration
------------

Since EWRL 2012 is an ICML workshop, the ICML-workshop fees have to be paid. There won’t be any additional EWRL specific fees.  
Registration [via ICML Workshops](http://icml.cc/2012/registration/)

Workshop Venue
--------------

Appleton Tower, LT 1

The poster sessions will be in the atrium of the Appleton Tower.

Scholarships
------------

Students can apply for financial support: Send an email to **_marc@ias.tu-darmstadt.de_** explaining why and how much financial support is required.

Keynote Speakers’ Abstracts
---------------------------

**Shie Mannor:** **Known Unknowns: Planning with Parameter Uncertainty**  
Planning when the model parameters are not fully known is a common problem encountered in operations research, control, and artificial intelligence. I will start with demonstrating why planning with parameter uncertainty is an important issue. I will then describe several approaches: Bayesian uncertainty model over the unknown parameters, a robust approach that takes a worst case view, and a frequentist approach. I will outline the advantages and disadvantages of each approach and discuss its potential to scale-up to large problems. I will finally discuss the challenges that are posed by a higher level of uncertainty, where the model itself rather than its parameters may not be fully known.

**Martin Riedmiller:** **Neural Architectures for Real World Reinforcement Learning**  
The research focus of the Machine Learning Lab at the University of Freiburg lies in building intelligent control architectures that can  
learn their behaviour entirely from scratch. Our aim is to build learning machines that perceive their environment, autonomously learn  
to generate internal representations and autonomously learn to make appropriate decisions to finally reach a predefined goal.

In my talk I will provide examples of how neural network based  
learning methods can be effectively applied to realize such control  
architectures. As one example, I will present some recent results on  
deep learning architectures for visual input based reinforcement  
learning.

**Richard Sutton:** **Verification in Artificial Intelligence**

**Drew Bagnell**: **Machine Learning with Multiple Guesses: Contextual Control Libraries**  
High-dimensional action spaces are an increasingly important in problems of reinforcement and imitation learning, robotics, and control.  
A popular approach to managing such difficulties in robotics uses a library of candidate “maneuvers” or “trajectories”. The library is either evaluated on a fixed number of candidate choices at runtime (e.g. path set selection for planning) or by iterating through a sequence of feasible choices until success is achieved (e.g. grasp selection). The performance of the library relies heavily on the content and order of the sequence of candidates. We propose a provably efficient method to optimize such libraries leveraging recent advances in optimizing sequence sub-modular functions.

An alternate approach to such problems is to directly attempt to predict the correct control action in a learning based approach, attempting to bypass the evaluation of a tremendous number of choices. Such methods, however, have no way to recover if the prediction is not a good one.  
In the second part of the talk, I will show an extension that yields a general approach to predict a sequence of potential actions based on the context (e.g., perceptual information, environment description, and goals). We take a simple, efficient, reduction-based approach where the choice and order of the items is established by repeatedly learning simple classifiers or regressors for each “slot” in the sequence. This approach can be thought of as capturing the notion of “predict then simulate”: checking multiple educated guesses in simulation and executing the most promising one. Finally we demonstrate the efficacy of the approaches on local trajectory optimization techniques, grasp library selection, and ground vehicle path set selection.

Joint work with Debadeepta Dey, Tommy Liu, and Martial Hebert.

Workshop Schedule
-----------------

**Saturday (June 30)**



* 08:30 – 09:00 : 09:00 – 09:15
  * COFFEE for arrival: Welcome
* 08:30 – 09:00 : 09:15 – 10:10
  * COFFEE for arrival: Invited Talk: Shie Mannor (“Known Unknowns”)
* 08:30 – 09:00 : 10:10 – 10:30
  * COFFEE for arrival: Shiau Hong Lim and Peter Auer: Autonomous Exploration For Navigating In MDPs
* 08:30 – 09:00 : 10:30 – 11:00 
  * COFFEE for arrival: COFFEE
* 08:30 – 09:00 : 11:00 – 11:15
  * COFFEE for arrival: Cosmin Paduraru, Doina Precup, Joelle Pineau and Gheorghe Comanici: A Study of Off-policy Learning in Computational Sustainability
* 08:30 – 09:00 : 11:15 – 11:30
  * COFFEE for arrival: Sergiu Goschin, Ari Weinstein, Michael Littman and Erick Chastain: Planning in Reward-Rich Domains via PAC Bandits
* 08:30 – 09:00 : 11:30 – 11:45
  * COFFEE for arrival: Michael Castronovo, Francis Maes, Raphael Fonteneau and Damien Ernst: Learning Exploration/Exploitation Strategies for Single Trajectory Reinforcement Learning
* 08:30 – 09:00 : 11:45 – 12:00
  * COFFEE for arrival: Pedro Ortega and Daniel Alexander Braun: Free Energy and the Generalized Optimality Equations for Sequential Decision Making
* 08:30 – 09:00 : 12:00 – 12:15
  * COFFEE for arrival: Amir-Massoud Farahmand, Doina Precup and Mohammad Ghavamzadeh: Generalized Classification-based Approximate Policy Iteration
* 08:30 – 09:00 : 12:15 – 12:30
  * COFFEE for arrival: Nicolas Heess, David Silver and Yee Whye Teh: Actor-Critic Reinforcement Learning with Energy-Based Policies
* 08:30 – 09:00 : 12:30 – 14:00
  * COFFEE for arrival: LUNCH
* 08:30 – 09:00 : 14:00 – 14:50
  * COFFEE for arrival: Invited Talk: Martin Riedmiller (“Neural Architectures for Real World Reinforcement Learning”)
* 08:30 – 09:00 : 14:50 – 15:10
  * COFFEE for arrival: David Silver: Gradient Temporal Difference Networks
* 08:30 – 09:00 : 15:10 – 15:30
  * COFFEE for arrival: Marc Deisenroth and Jan Peters: Solving Nonlinear Continuous State-Action-Observation POMDPs for Mechanical Systems with Gaussian Noise
* 08:30 – 09:00 : 15:30 – 16:00
  * COFFEE for arrival: COFFEE
* 08:30 – 09:00 : 16:00 – 17:30
  * COFFEE for arrival: Poster Session I
* 08:30 – 09:00 : 18:30 – 
  * COFFEE for arrival: Banquet


**Sunday (July 1)**



* 08:30 – 09:00: 09:00 – 09:50
  * COFFEE for arrival: Invited Talk: Drew Bagnell (“Machine Learning with Multiple Guesses: Contextual Control Libraries”)
* 08:30 – 09:00: 09:50 – 10:05
  * COFFEE for arrival: Nikos Vlassis, Michael Littman and David Barber: Stochastic POMDP controllers: How easy to optimize?
* 08:30 – 09:00: 10:05 – 10:20
  * COFFEE for arrival: Hado van Hasselt: Pre-learning in Generalized MDPs to Speed up Learning
* 08:30 – 09:00: 10:30 – 11:00
  * COFFEE for arrival: COFFEE
* 08:30 – 09:00: 11:00 – 12:30
  * COFFEE for arrival: Poster Session II
* 08:30 – 09:00: 12:30 – 14:00
  * COFFEE for arrival: LUNCH
* 08:30 – 09:00: 14:00 – 14:50
  * COFFEE for arrival: Invited Talk: Richard Sutton  (“Verification in Artificial Intelligence”)
* 08:30 – 09:00: 14:50 – 15:10
  * COFFEE for arrival: Gabriel Dulac-Arnold, Ludovic Denoyer, Philippe Preux and Patrick Gallinari: Fast Reinforcement Learning with Large Action Sets using Error-Correcting Output Codes for MDP Factorization
* 08:30 – 09:00: 15:10 – 15:25
  * COFFEE for arrival: Michal Valko, Mohammad Ghavamzadeh and Alessandro Lazaric: Semi-Supervised Inverse Reinforcement Learning
* 08:30 – 09:00: 15:30 – 16:00
  * COFFEE for arrival: COFFEE
* 08:30 – 09:00: 16:00 – 16:15
  * COFFEE for arrival: Abdeslam Boularias, Oliver Kroemer and Jan Peters: Structured Apprenticeship Learning
* 08:30 – 09:00: 16:15 – 16:30
  * COFFEE for arrival: Mahdi Milani Fard, Yuri Grinberg, Joelle Pineau and Doina Precup: Bellman Error Based Feature Generation Using Random Projections
* 08:30 – 09:00: 16:30 – 16:45
  * COFFEE for arrival: Edouard Klein, Bilal Piot, Matthieu Geist and Olivier Pietquin: Structured Classification for Inverse Reinforcement Learning
* 08:30 – 09:00: 16:45 – 17:00
  * COFFEE for arrival: Jan Hendrik Metzen: Online Skill Discovery using Graph-based Clustering
* 08:30 – 09:00: 17:00 – 17:15
  * COFFEE for arrival: Alborz Geramifard, Stefanie Tellex, David Wingate, Nicholas Roy and Jonathan How: A Bayesian Approach to Finding Compact Representations for Reinforcement Learning
* 08:30 – 09:00: 17:15 – 17:30
  * COFFEE for arrival: Arthur Guez, David Silver and Peter Dayan: Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search
* 08:30 – 09:00: 17:30 – 17:45
  * COFFEE for arrival: Closing Remarks


Accepted Papers for Presentation at EWRL 2012
---------------------------------------------

Nikos Vlassis, Michael Littman and David Barber:  
[Stochastic POMDP controllers: How easy to optimize?](https://ewrl.wordpress.com/wp-content/uploads/2011/12/vlassis12ewrl.pdf)  

Jan Hendrik Metzen:  
Model-based Direct Policy Search for Skill Learning in Continuous Domains

Dotan Di Castro, Aviv Tamar and Shie Mannor:  
Policy Gradients with Variance Related Risk Criteria

Yevgeny Seldin, Peter Auer, Yasin Abbasi-Yadkori and Csaba Szepesvári:  
Evaluation and Analysis of the Performance of the EXP3 Algorithm in Stochastic Environments

Marek Petrik:  
Approximate Dynamic Programming By Minimizing  Distributionally Robust Bounds

Takaki Makino and Johane Takeuchi:  
Apprenticeship Learning for Model Parameters of Partially Observable Environments

Nicolas Heess, David Silver and Yee Whye Teh:  
Actor-Critic Reinforcement Learning with Energy-Based Policies

Hado van Hasselt:  
Pre-learning in Generalized MDPs to Speed up Learning

David Silver and Kamil Ciosek:  
Compositional Planning Using Optimal Option Models

Program Committee
-----------------

Abdeslam Boularias  
Adam White  
Alborz Geramifard  
Alessandro Lazaric  
Amir-massoud Farahmand  
Andre Damotta Salles Barreto  
Andrew McHutchon  
Bert Kappen  
Bradley Knox  
Byron Boots  
Carlos Diuk Wasser  
Christian Daniel  
Christian Igel  
Csaba Szepesvari  
Damien Ernst  
David Silver  
Doina Precup  
Dvijotham Krishnamurthy  
Emma Brunskill  
Evangelos Theodorou  
Fernand Fernandez  
Francisco Melo  
Gerhard Neumann  
Hado van Hasselt  
Jan Peters  
Jens Kober  
Jose Antonio Martin H.  
Jun Morimoto  
Katharina Mülling  
Kristian Kersting  
Manuel Lopes  
Marc Deisenroth  
Marco Wiering  
Martijn van Otterlo  
Martin Riedmiller  
Masashi Sugiyama  
Matthew Hoffman  
Matthew Robards  
Matthieu Geist  
Michal Valko  
Mohammad Ghavamzadeh  
Nikos Vlassis  
Odalric-Ambrym Maillard  
Oliver Kroemer  
Olivier Pietquin  
Pedro Ortega  
Peter Auer  
Peter Dayan  
Peter Sunehag  
Philipp Hennig  
Philippe Preux  
Remi Munos  
Ronald Ortner  
Shivaram Kalyanakrishnan  
Stephane Ross  
Teodor Moldovan  
Thomas Furmston  
Thomas J. Walsh  
Thomas Rückstieß  
Tobias Jung  
Tobias Lang  
Todd Hester  
Tom Erez  
Tom Schaul  
Verena Heidrich-Meisner  
Yuri Grinberg  
Zhikun Wang  
Zico Kolter

Additional Reviewers
--------------------

Christoph Dann  
Javier Garcia Polo

Sponsors
--------

[![](https://ewrl.wordpress.com/wp-content/uploads/2011/12/pascal2.png?w=300&h=105 "PASCAL2")](http://pascallin2.ecs.soton.ac.uk/)

[![](https://ewrl.wordpress.com/wp-content/uploads/2011/12/msr_logo.jpg?w=181&h=50 "MSR_Logo")](https://ewrl.wordpress.com/wp-content/uploads/2011/12/msr_logo.jpg)
